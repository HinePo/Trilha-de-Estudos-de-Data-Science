# Trilha de Estudos de Ciência de Dados

Este é um repositório em constante construção e atualização. Adiciono aqui técnicas de estudo e fontes que considero boas para o aprendizado de ciência de dados, com o objetivo de manter recursos organizados para consulta e ajudar quem se interessa pelo tema. O conteúdo aqui compilado vai do básico ao avançado.

# Sumário

- [Como estudar](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#como-estudar)
- [Python and Data Analysis basics](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#python-and-data-analysis-basics)
- [Data Visualization](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#data-visualization)
- [Machine Learning - Teoria](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#machine-learning---teoria)
- [Machine Learning - Prática](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#machine-learning---prática)
- [Time Series](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#time-series)
- [Deep Learning - Neural Networks](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#deep-learning---neural-networks)
- [Transformers](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#transformers)
- [NLP - Natural Language Processing](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#nlp---natural-language-processing)
- [LLMs](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#llms)
- [Computer Vision](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#computer-vision)
- [RecSys](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#recsys)
- [Marketing Mix Modeling](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#marketing-mix-modeling)
- [Survival Analysis](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#survival-analysis)
- [Deploy](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#deploy)
- [MLOps](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#mlops)
- [Youtube channels](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#youtube-channels)
- [Perfis no twitter](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#perfis-no-twitter)

# Como estudar

- Criar um documento (Word, Notion, Evernote etc) pessoal com a sua organização do que vc já aprendeu/estudou e o que planeja aprender/estudar, de preferência organizado por mês ou bimestre. Procurar manter este doc atualizado
- Instalar a extensão [video speed controller](https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk) no google chrome (funciona em qualquer vídeo tocado pelo chrome browser), e aprender a usar
- Sempre estude do geral para o específico: [top-down learning](https://en.wikipedia.org/wiki/Top-down_and_bottom-up_design)
- Para cada dose de teoria, uma dose de prática: [problem-based learning](https://en.wikipedia.org/wiki/Problem-based_learning)
-	Ao entrar em um assunto novo, ver um ou dois vídeos de ~10 min no youtube, pesquisar sobre o tema focando em material escrito, e estudar aplicações
-	Evitar ficar muito tempo na parte teórica: Qualquer assunto novo tem suas aplicações via bibliotecas específicas. Se familiarizar com a documentação é o primeiro passo para aplicar o que aprendeu
-	O segundo passo é a aplicação e uso, parte prática: Resolver problemas usando IA: Pesquisar aplicações no Kaggle (notebooks), fazer o fork (Copy and Edit), adicionar ideias.
-	Evitar tentar reinventar a roda: aproveitar os códigos que já existem
- Adicionar aplicação ao seu repositório pessoal (público ou privado - kaggle ou github) de forma organizada para que você possa facilmente consultá-la no futuro

### ML twitter

- Em 2022, nem os papers do arxiv e nem blogposts conseguem acompanhar a velocidade no avanço no ML/DL. As publicações acontecem de forma muito mais rápida e dinâmica no twitter, que é absolutamente fundamental pra quem quer acompanhar o estado da arte. [Exemplo](https://twitter.com/bneyshabur/status/1570843225585504256)
- O twitter deve ser utilizado como ferramenta de estudo e atualização. Funciona muito bem como dose diária de aprendizado, e ajuda muito a acompanhar o trabalho de outros cientistas de dados e pesquisadores.
- Ferramenta essencial não só para o acompanhamento dos avanços na área de ciência de dados e papers publicados, mas também para a absorção de dicas e experiências compartilhadas sobre casos reais de DS na indústria e área de negócios.
- Nenhuma outra plataforma te entrega a informação de forma tão rápida e curada.
- Ver sugestões de perfis a seguir no final deste documento.

### Ferramentas
- Focar em Google Colab e Kaggle notebooks.
- No futuro, é interessante conhecer IDEs como VS Code, PyCharm e Spyder.

### Panorama de DS no mercado

- [Data Scientist: The Sexiest Job of the 21st Century - artigo out/2012](https://hbr.org/2012/10/data-scientist-the-sexiest-job-of-the-21st-century)
- [Is Data Scientist Still the Sexiest Job of the 21st Century? - artigo jul/2022](https://hbr.org/2022/07/is-data-scientist-still-the-sexiest-job-of-the-21st-century)

### Áreas de dados

![Data fields](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/imgs/blob/main/data_fields.jpg)


![Data fields](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/imgs/blob/main/data sequence.png)

# Python and Data Analysis basics
- [Never memorize code](https://www.youtube.com/watch?app=desktop&v=AavXBoxTCIA)
- [3 Tips to Build a Career in Machine Learning](https://www.youtube.com/watch?v=plXM4pNSYXs)
- [How to learn data science smartly](https://www.youtube.com/watch?app=desktop&v=csG_qfOTvxw)
- [Pandas tutorial - kaggle learn](https://www.kaggle.com/learn/pandas) - Fazer o fork dos notebooks e aprender praticando
- [Use pandas like a pro](https://www.youtube.com/watch?v=_gaAoJBMJ_Q)
- [Learn Pandas with pokemons](https://www.kaggle.com/ash316/learn-pandas-with-pokemons)
- [Handling Missing Data](https://www.kaggle.com/robikscube/handling-with-missing-data-youtube-stream)

### Data Analysis workflow - entender e praticar as etapas básicas:

- Importar e ler csv, criar dataframe
- Checar tipos de variáveis (data types): numéricas e categóricas
- Plots básicos
- Analisar missing values (valores faltantes), tomar decisões sobre o que fazer com eles
- Analisar outliers, decidir o que fazer com eles
- Análise univariada, bivariada, multivariada (variáveis categóricas e numéricas)
- Feature Engineering (criação de variáveis)
- Deixar dados prontos para eventual modelagem de IA

### Machine Learning workflow - entender e praticar as etapas básicas:

- Definir Features and Target (if it is a supervised problem)
- Preprocess: Scaling and categorical encoders
- Check Target distributions
- Check features distributions, normalize them if needed
- Split train/validation datasets: Definir estratégia confiável de validação dos modelos (cross-validation strategy)
- Definir métricas de avaliação dos modelos
- Criar um baseline simples ***sem*** usar machine learning, e avaliar usando as métricas definidas
- Choose algorithm, train and validate model: create simple baseline ***using*** a ML model: [Lei de Gall](https://twitter.com/radekosmulski/status/1614385689424187392)
- Evaluate model (fora da etapa de modelagem: cruzamento com business KPIs)
- Melhorar baseline: criar variáveis melhores (feature engineering), tunar hiperparâmetros, testar outros algoritmos, treinar e avaliar de novo
- [Missing values & XGBoost](https://twitter.com/svpino/status/1559510300428156929)
- [OOF analysis & Error analysis](https://www.kaggle.com/competitions/siim-isic-melanoma-classification/discussion/175614)
- Experimentos de Feature selection e preprocessing: diferentes sets de features e diferentes etapas de pré-processamento
- Melhorar explicabilidade
- Ensemble: combinar modelos para aumentar performance, estabilidade e poder de generalização

# Data Visualization

- [Python Graph Gallery](https://www.python-graph-gallery.com/)
- [Visual Reference](https://www.sqlbi.com/wp-content/uploads/videotrainings/dashboarddesign/visuals-reference-may2017-A3.pdf)
- [Séries de notebooks de visualização](https://www.kaggle.com/residentmario/univariate-plotting-with-pandas) - ao final de cada notebook tem um link para o próximo
- [Data Analysis - Brazilian Society (PNAD)](https://www.kaggle.com/hinepo/pnad-data-analysis) - @hinepo
- [Rio Temperature Analysis](https://www.kaggle.com/hinepo/is-rio-getting-hotter-seasons-analysis/) - @hinepo
- [Power BI playlists](https://www.youtube.com/c/HashtagTreinamentos/playlists?app=desktop)
- [Power BI - Leonardo Karpinski](https://www.youtube.com/c/AprendaPowerBI)
- [Power BI - Karine Lago](https://www.youtube.com/c/KarineLago/playlists?app=desktop)
- [Power BI + DAX + Projetos na prática - Curso Udemy](https://www.udemy.com/course/curso-de-powerbi-desktop-dax/)
- [graphviz - exemplos](https://graphviz.readthedocs.io/en/stable/examples.html)

# Machine Learning - Teoria

- [Supervised x Unsupervised Learning](https://www.youtube.com/watch?app=desktop&v=1FZ0A1QCMWc)
- [Supervised x Unsupervised Learning: applications](https://www.youtube.com/watch?app=desktop&v=rHeaoaiBM6Y)
- Pesquisar sobre Overfitting e Underfitting, ver vídeos e gráficos
- [Cross Validation](https://www.youtube.com/watch?app=desktop&v=fSytzGwwBVw)
- [Cross Validation - scikit docs](https://scikit-learn.org/stable/modules/cross_validation.html)
- Pesquisar sobre Cross Validation para Time Series (como evitar contaminação de dados do futuro pro passado, data leakage, train/test contamination...)
- [Kaggle courses](https://www.kaggle.com/learn)
- [pdf do livro do Abhishek Thakur](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf) - disponível na Amazon tb
- [Statquest - Vídeos sobre conceitos, teoria e matemática de algoritmos e ML](https://www.youtube.com/c/joshstarmer/playlists?app=desktop)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) - Muito importante ler todo o item 1
- [Scikit-learn Pre-processing](https://scikit-learn.org/stable/modules/preprocessing.html)
- Pesquisar sobre "Feature Engineering" (criação de variáveis)
- Pesquisar sobre métricas e como avaliar modelos:
  - Classificação: Accuracy, ROC AUC, f1-score, recall, precision, [MCC](https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a)
  - Regressão: RMSE, MSE, MAE, MAPE, R²
    - [Transforming skewed data](https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0) - como tratar o viés nos dados
- Outros conceitos importantes: Pesquisar sobre Boosting (XGBoost, LGBM, Catboost, GBM), Bagging, Split train/test, data leakage, time series, feature importances, ensemble...
- Imbalanced learning:
  - [4 ways to balance classes](https://www.kaggle.com/competitions/icr-identify-age-related-conditions/discussion/412507)
  - downsampling/upsampling
  - [imblearn](https://opendatascience.com/strategies-for-addressing-class-imbalance/)
  - [Oversampling x Undersampling](https://www.kdnuggets.com/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html)
  - [Resampling example](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)
  - [SMOTE for classification example](https://www.kaggle.com/shrutimechlearn/pokemon-classification-and-smote-for-imbalance/notebook)
  - [Stop Using SMOTE to Treat Class Imbalance](https://towardsdatascience.com/stop-using-smote-to-treat-class-imbalance-take-this-intuitive-approach-instead-9cb822b8dc45)

# Machine Learning - Prática
- [Kaggle's 30 Days of ML](https://www.youtube.com/playlist?app=desktop&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N) - Abhishek Thakur
- [Data Visualization & Income Prediction for Brazilians](https://www.kaggle.com/hinepo/pnad-income-prediction) - @hinepo
- [Lazy Predict](https://www.kaggle.com/hinepo/pnad-lazy-predict) - @hinepo
- [Heart disease and ensembling](https://www.kaggle.com/hinepo/ensemble-to-predict-diabetes-100-acc-on-val-data) - @hinepo
- [Fair Learn docs](https://fairlearn.org/v0.7.0/quickstart.html) - ajuda a fazer OOF analysis
- [SHAP (SHapley Additive exPlanations)](https://github.com/helenaEH/SHAP_tutorial)
- [Target encoding - kaggle Learn](https://www.kaggle.com/code/ryanholbrook/target-encoding)
- [Target encoding - blog post](https://towardsdatascience.com/why-you-should-try-mean-encoding-17057262cd0)
- [11 Categorical Encoders and Benchmark - kaggle](https://www.kaggle.com/code/subinium/11-categorical-encoders-and-benchmark)
- Browse kaggle, ver notebooks e datasets dos assuntos que te interessam
- Fazer forks de notebooks do kaggle (Copy and Edit), testar hipóteses e técnicas
- Falar com as pessoas do kaggle, comentar e postar, fazer parte da comunidade
- Competições 'Getting Started': estudar notebooks com bom score, e usar técnicas e conceitos aprendidos para criar o seu próprio. Estudar notebooks com score médio, comparar com os de score bom, e entender o que causou a melhora na pontuação. Recomendo no mínimo uns 10 dias de estudo para cada uma das competições abaixo:
  - [Titanic Classification](https://www.kaggle.com/c/titanic)
  - [House Prices Regression](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
  - [Predict Future Sales](https://www.kaggle.com/c/competitive-data-science-predict-future-sales)
  - [Tabular Playground Series](https://www.kaggle.com/c/tabular-playground-series-sep-2021)
  - Nível avançado: competições reais (valendo prêmios)

### Algorithm Optimization & Tuning techniques

- [Optuna library](https://optuna.org/)
- [Optuna example - notebook](https://www.kaggle.com/hinepo/extra-trees-optimization-with-optuna) - @hinepo
- [Optuna example](https://www.youtube.com/watch?v=m5YSKPMjkrk&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N&index=23) - Abhishek Thakur
- [Optuna official tutorial](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
- [Tuning techniques](https://www.youtube.com/watch?v=5nYqK-HaoKY) - Abhishek Thakur

# Time Series

- [Rob Mulla tutorial](https://www.youtube.com/watch?v=vV12dGe_Fho)
- [3 methods for Time Series validation](https://forecastegy.com/posts/3-essential-methods-to-do-time-series-validation-in-machine-learning/)
- [Error Analysis for Time Series - Mark Tanenholtz](https://twitter.com/marktenenholtz/status/1509500787189190658)
- [More time series tips from Mark](https://twitter.com/marktenenholtz/status/1633090131514241026?t=j_GHtqW4tGlL-7mYP7xLrQ&s=08)
- [Even more time series tips from Mark](https://twitter.com/marktenenholtz/status/1661372273888993285?t=zzPy9Meia-gtr2kK_tFJsg&s=08)
- [Classic features for time series models](https://twitter.com/marktenenholtz/status/1667670055897927681)
- [Techniques for working with time series data](https://towardsdatascience.com/a-collection-of-must-know-techniques-for-working-with-time-series-data-in-python-7c01d199b184)
- [Time Series basic concepts](https://medium.com/swlh/time-series-analysis-7006ea1c3326)
- [Time Series - Youtube playlist](https://www.youtube.com/playlist?list=PL98nY_tJQXZmT9ZB59T0lsx0ZzzLrYdX4)
- [pmdarima](http://alkaline-ml.com/pmdarima/), [statsmodels](https://www.statsmodels.org/stable/index.html), ARIMA, SARIMA, [prophet](https://facebook.github.io/prophet/docs/quick_start.html#python-api), theta model
- [Time-series prediction with XGBoost](https://www.linkedin.com/feed/update/urn:li:activity:7069236572063186944/)
- [XGBoost for time series example](https://mlwhiz.com/blog/2019/12/28/timeseries/?utm_campaign=using-gradient-boosting-for-time-series-prediction-tasks&utm_medium=social_link&utm_source=missinglettr)
- **Tabular** Machine learning approach para time series:
  - Formating
  - Lag features
  - Tail(1)
  - Validation strategy, types of leakage
  - Evaluation
  - [Riiid](https://www.kaggle.com/competitions/riiid-test-answer-prediction) - competição excelente para treinar time series ML
- Ver notebooks [desta competição](https://www.kaggle.com/competitions/competitive-data-science-predict-future-sales/code?competitionId=8587&sortBy=voteCount)  

# Deep Learning - Neural Networks

Principais conceitos e keywords a pesquisar e aprender: tensors, gradient descent, automatic differentiation, forward pass, backpropagation, layers, batch, epoch, iteration, optimizer step, scheduler step, vanishing gradients, exploding gradients, transfer learning (fine-tuning & feature extraction)...


Basics:
- [Neural Networks - 3Blue1Brown Playlist (~1h)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Aula Intro de DL - Lex Friedman](https://www.youtube.com/watch?app=desktop&v=O5xeyoRL95U)
- [Deep Learning & Bugs - discussion](https://twitter.com/SebastienBubeck/status/1571887980046065667)
- [Exploding and Vanishing gradients](https://www.analyticsvidhya.com/blog/2021/06/the-challenge-of-vanishing-exploding-gradients-in-deep-neural-networks/)
- [Regression Networks - The Magic of No Dropout](https://www.kaggle.com/competitions/commonlitreadabilityprize/discussion/260729)

Frameworks:
- [Pytorch vs Tensorflow in 2022](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/)

Keras:
- [Keras Sequential class](https://keras.io/api/models/sequential/)
- [Finetuning - Keras tutorial](https://keras.io/guides/transfer_learning/)
- [Keras - code examples](https://keras.io/examples/)

Tensorflow:
- [Tensorflow guide - kaggle learn](https://www.kaggle.com/learn-guide/tensorflow)

Pytorch:
- [Pytorch DataLoader animation](https://twitter.com/_ScottCondron/status/1363494433715552259?t=whYsVwzXnMxZ-pHuQn8FMQ&s=08)
- [Finetuning x Feature Extraction - pytorch docs and examples](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)
- [Pytorch - Abhishek Thakur playlist and tutorials](https://www.youtube.com/playlist?app=desktop&list=PL98nY_tJQXZln8spB5uTZdKN08mYGkOf2)
- [Pytorch - torch.nn](https://pytorch.org/docs/stable/nn.html)
- [Vídeo aulas com code examples (pytorch)](https://sebastianraschka.com/blog/2021/dl-course.html)

Um estudo muito útil e proveitoso é comparar e olhar em paralelo as documentações de Quick Start do Keras, do Tensorflow e do Pytorch. A lógica é bem parecida e existem muitas analogias:
- [Keras Quick Start](https://www.tensorflow.org/tutorials/quickstart/beginner)
- [Tensorflow Quick Start](https://www.tensorflow.org/tutorials/quickstart/advanced)
- [Pytorch Quick Start](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)

Principais tipos de camadas (layers):
- Dense & Linear (fully connected)
- Activation functions (ReLU, LeakyReLU, SELU, PReLU, Tanh, Softmax, Sigmoid....)
- Conv (Convolutional)
- Flatten
- BatchNorm, LayerNorm, RMSNorm
- LSTM (Long Short Term Memory), BiLSTM
- GRU (Gated Recurrent Unit - Short Term Memory), BiGRU
- Dropout
- Pooling (Max, Mean, Average, Generalized Mean etc)
- Concatenate

### Papers - Why and When Deep Learning?

- [Do We Really Need Deep Learning Models for Time Series Forecasting? - paper oct/2021](https://arxiv.org/pdf/2101.02118.pdf)
- [Tabular Data: Deep Learning Is Not All You Need - paper nov/2021](https://arxiv.org/pdf/2106.03253.pdf)

### Extra:

- [JAX docs](https://jax.readthedocs.io/en/latest/) 

"JAX is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more."

- [JAX - Quick Start](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)

"JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research."

[JAX é um projeto open source do Google](https://github.com/google/jax) com o objetivo de criar uma API simples e backend eficiente para cálculos de deep learning. Tem crescido em popularidade e sido considerada muito promissora por pesquisadores. Imagina-se que em alguns anos será um concorrente direto do Pytorch (na área de pesquisa), e também deverá substituir o backend do tensorflow (na área de aplicações). Há quem chame o JAX de  "tensorflow 3", e já existem planos para criação de uma API high level para JAX, adaptando a biblioteca Keras para usar JAX como backend. Portanto, é interessante conhecer.

- [JAX guide - kaggle learn](https://www.kaggle.com/learn-guide/jax)

# Transformers

Os Transformers e o Attention Mechanism, propostos em 2017 por Vaswani - Google Brain no paper Attention Is All You Need, são, até hoje, a maior revolução que o mundo do Deep Learning já passou. Vale a pena estudá-los com atenção (pun intended 😆), pois eles são o estado da arte em redes neurais hoje em dia para a maioria dos tasks, e pelo visto continuarão sendo por bastante tempo.

Transformers mostraram que não é preciso usar camadas LSTM para fazer tasks de NLP no estado da arte, e também não precisamos de camadas de Convolução para fazer CV (Computer Vision) no estado da arte. Attention Is All You Need.

### Papers

- [How to read papers - twitter thread](https://twitter.com/marktenenholtz/status/1498644231149142025)
- [Attention Is All You Need - paper dec/2017](https://arxiv.org/pdf/1706.03762.pdf)
- [BERT - paper may/2019](https://arxiv.org/pdf/1810.04805.pdf)
- [RoBERTa - paper jul/2019](https://arxiv.org/pdf/1907.11692.pdf)
- [SBERT - paper aug/2019](https://arxiv.org/pdf/1908.10084.pdf) - Sentence Transformers
- [TaBERT - paper may/2020](https://arxiv.org/pdf/2005.08314v1.pdf) - Learning Joint Representations over Textual and Tabular Data
- [T5: Text-To-Text Transfer Transformer - paper jul/2020](https://arxiv.org/pdf/1910.10683.pdf)
- [Longformer - paper dec/2020](https://arxiv.org/pdf/2004.05150.pdf) - Local Attention
- [ViT - paper jun/2021](https://arxiv.org/pdf/2010.11929.pdf) - Vision Transformers
- [Swin Transformer - paper aug/2021](https://arxiv.org/pdf/2103.14030.pdf) - Shifted Window based Self-Attention
- [DeBERTa - paper oct/2021](https://arxiv.org/pdf/2006.03654.pdf) - Disentangled Attention

### Outras fontes

- [BERT Attention Mechanism](https://peltarion.com/blog/data-science/self-attention-video)
- [Illustrated Guide to Transformers](https://www.youtube.com/watch?app=desktop&v=4Bdc55j80l8)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- [Attention implementation in torch from scratch - twitter thread](https://twitter.com/abhi1thakur/status/1470406419786698761) - Abhishek Thakur
- [Attention implementation in torch from scratch - twitter thread 2](https://twitter.com/abhi1thakur/status/1471126502112698368) - Abhishek Thakur
- [Transformers from Scratch](https://e2eml.school/transformers.html) - explicação visual e detalhada
- [Awesome Self-Supervised Learning - github repo](https://github.com/jason718/awesome-self-supervised-learning)


# NLP - Natural Language Processing

Principais conceitos e keywords a conhecer: n-grams, CountVectorizer, TF-IDF, BOW (Bag of Words), CBOW (Continuous Bag of Words), Word2vec, FastText (facebook model), GloVe (Global Vectors), BERT, RoBERTa, Hugging Face....

- [A brief timeline of NLP from Bag of Words to the Transformer family](https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56)
- [The Illustrated Word2vec - A Gentle Intro to Word Embeddings in Machine Learning](https://www.youtube.com/watch?v=ISPId9Lhc1g)
- [BERT tutorial by Abhishek Thakur](https://www.youtube.com/playlist?app=desktop&list=PL98nY_tJQXZl0WwsJluhc6tGrKWCX2suH)
- [Resumo Hugging face library - 15 min video](https://www.youtube.com/watch?v=QEaBAZQCtwE)
- [Hugging face - finetune a pretrained model: Trainer, native Pytorch, native Tensorflow](https://huggingface.co/docs/transformers/training)
- [Hugging Face course](https://huggingface.co/course/chapter1/1) - excelente curso. HF é o melhor ecossistema de NLP e continuará sendo por muitos anos
- [Hugging Face tutorial - video (30 min)](https://www.youtube.com/watch?v=DQc2Mi7BcuI])
- [10 Things You Need to Know About BERT and Transformer Architecture](https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape)
- [A Survey of Transformers - paper jun/2021](https://arxiv.org/pdf/2106.04554.pdf)
- [Transformers in Tensorflow](https://www.tensorflow.org/text/tutorials/transformer#setup)
- [NLP guide - kaggle learn](https://www.kaggle.com/learn-guide/natural-language-processing)

# LLMs

Large Language Models

- Conceitos básicos
  - [State of GPT - Andrej Karpathy](https://www.youtube.com/watch?v=bZQun8Y4L2A)
  - [Vector databases and use cases](https://www.youtube.com/watch?v=dN0lsF2cvm4)
  - [Bits and Bytes + Hugging Face integration](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
  - [RLHF: Reinforcement Learning from Human Feedback](https://huyenchip.com/2023/05/02/rlhf.html)
  - [Understanding PEFT (Parameter-Efficient Finetuning)](https://lightning.ai/pages/community/article/understanding-llama-adapters/)
  - [LoRA](https://lightning.ai/pages/community/tutorial/lora-llm/)
  - [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/pdf/2305.14314.pdf)

- LangChain e aplicações
  - [LangChain - Quickstart](https://python.langchain.com/en/latest/getting_started/getting_started.html)
  - [Harry Potter Question Answering with LangChain](https://www.kaggle.com/code/hinepo/harry-potter-question-answering-with-langchain/notebook) - @hinepo
  - [Summarize YouTube videos with Langchain](https://www.kaggle.com/code/hinepo/summarize-youtube-videos-with-langchain/notebook) - @hinepo
  - [Beginner’s Guide to Building LLM-Powered Applications](https://towardsdatascience.com/getting-started-with-langchain-a-beginners-guide-to-building-llm-powered-applications-95fc8898732c)
  - [LangChain Cookbook Part 1 - Data Independent](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%201%20-%20Fundamentals.ipynb)
  - [LangChain Cookbook Part 2 - Data Independent](https://github.com/gkamradt/langchain-tutorials/blob/main/LangChain%20Cookbook%20Part%202%20-%20Use%20Cases.ipynb)
  - [LangChain - youtube playlist recomendada na documentação](https://www.youtube.com/watch?v=_v_fgW2SkkQ&list=PLqZXAkvF1bPNQER9mLmDbntNfSpzdDIU5&index=3)
  - [Multiple-document retriever](https://www.youtube.com/watch?v=9ISVjh8mdlA)
  - [Talk to your data](https://www.youtube.com/watch?v=xQ3mZhw69bc)
  - [Chatbot with Langchain, ChatGPT, Pinecone, and Streamlit](https://blog.futuresmart.ai/building-an-interactive-chatbot-with-langchain-chatgpt-pinecone-and-streamlit)
  - [Chatbot memory types in LangChain](https://twitter.com/Luc_AI_Insights/status/1662549087458144257?t=EsgW6ztVSZ4K0UCApf19WA&s=08)
  - [Retrieval augmentation tips](https://twitter.com/waydegilliam/status/1663631626440671232?t=VX8IcvpZDXr-CgXMv_rCFA&s=08)

- Finetuning LLMs
  - [Fine-tuning LLMs with PEFT and LoRA](https://www.youtube.com/watch?v=Us5ZFp16PaU)
  - [A Step-by-Step Guide to Fine-Tuning Your Dolly Model (tutorial)](https://www.youtube.com/watch?v=8lBFPHltGuQ)
  - [How to finetune your own Alpaca 7B](https://www.youtube.com/watch?v=LSoqyynKU9E)

- LLMs in Production
  - [Building LLM applications for production](https://huyenchip.com/2023/04/11/llm-engineering.html)

# Computer Vision

### OpenCV
- [Image Processing with OpenCV and Python](https://www.youtube.com/watch?v=kSqxn6zGE0c)
- [How to work with video data](https://www.youtube.com/watch?v=AxIc-vGaHQ0)
- [OpenCV Tutorials](https://github.com/murtazahassan/OpenCV-Python-Tutorials-and-Projects)
- [Image and Video processing - Great Barrier Reef](https://www.kaggle.com/code/soumya9977/learning-to-sea-underwater-img-enhancement-eda/notebook)

### Yolo videos and examples

- [Yolo Introduction](https://www.youtube.com/watch?app=desktop&v=4eIBisqx9_g)
- [Face mask yolo-v5 example](https://www.youtube.com/watch?v=12UoOlsRwh8)
- [Yolo v5 - Great Barrier Reef - Train](https://www.kaggle.com/code/awsaf49/great-barrier-reef-yolov5-train#Tensorflow---Help-Protect-the-Great-Barrier-Reef)
- [Yolo v5 - Great Barrier Reef - Inference](https://www.kaggle.com/code/awsaf49/great-barrier-reef-yolov5-infer/notebook#%F0%9F%93%A6-YOLOv5)
- [Yolo v5 tutorial - notebook](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb)
- [Yolo tutorial by Abhishek Thakur](https://www.youtube.com/watch?app=desktop&v=NU9Xr_NYslo)
- [How to Train YOLOv8 Object Detection on a Custom Dataset - using Roboflow](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/)
- [How to Train YOLOv8 Object Detection on a Custom Dataset - using folder structure and LabelImg](https://medium.com/augmented-startups/train-yolov8-on-custom-data-6d28cd348262)
- [YOLOv8 segmentation in 12 minutes](https://www.youtube.com/watch?v=pFiGSrRtaU4)
- [Yolo v8 - Detecting and Counting vehicles](https://www.kaggle.com/code/paulojunqueira/yolo-v8-vehicles-detecting-counting/notebook) - Paulo Junqueira


### Yolo papers and repos

- [Yolo v5 github repo - ultralytics](https://github.com/ultralytics/yolov5)
- [Yolo v5 - paper jul/2021](https://arxiv.org/pdf/2104.13634.pdf)
- [Yolo v6 - github repo - meituan](https://github.com/meituan/YOLOv6)
- [Yolo v7 - github repo - WongKinYiu](https://github.com/WongKinYiu/yolov7)
- [Yolo v7 - paper jul/2022](https://arxiv.org/pdf/2207.02696.pdf)
- [Yolov v8 github repo - Ultralytics](https://github.com/ultralytics/ultralytics)
- [Yolov8 Ultralytics Docs](https://docs.ultralytics.com/tasks/detection/)


### Basics

- [Computer Vision History by Andrej Karpathy](https://www.youtube.com/watch?v=u6aEYuemt0M&t=1460s) (até 2016)
- [Kernel size in convolution layers](https://medium.com/analytics-vidhya/significance-of-kernel-size-200d769aecb1)
- [Digit Recognizer: Getting Started Competition](https://www.kaggle.com/c/digit-recognizer) - ‘Hello World’ do mundo de CV: Estudar vários notebooks com bom score, e depois criar o seu misturando várias técnicas que vc achou promissoras em outros notebooks, tentando melhorar o score do baseline. Recomendo no mínimo uns 10 dias de estudo para essa competição.
- [chatGPT resolvendo MNIST usando keras](https://twitter.com/svpino/status/1626204842095161344)
- [Pytorch tutorial for image classification](https://www.kaggle.com/hinepo/pytorch-tutorial-cv-99-67-lb-99-26) - @hinepo
- [Ensemble for image classification](https://www.kaggle.com/hinepo/ensemble-by-probabilities-lb-99-43) - @hinepo
- [Transfer learning for CV guide - kaggle learn](https://www.kaggle.com/learn-guide/transfer-learning-cv)


### Pytorch image models (timm)

O que o Hugging Face é para NLP é análogo ao que a biblioteca timm é para computer vision: um ecossistema open source, consolidado e no estado da arte, que disponibiliza uma API simples e unificada para uso de modelos, além de centenas de excelentes modelos multi-propósito (multi-task, general purpose models), já pré-treinados durante semanas em GPUs e TPUs de dezenas de milhares de dólares, todos prontos para usarmos apenas adicionando uma última camada na rede neural para atender ao nosso task/problema. Isso se chama feature extraction, e evita que tenhamos que treinar esses modelos gigantes from scratch.

- [timm tutorial](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)
- [timm: pytorch image models](https://github.com/rwightman/pytorch-image-models/tree/master/timm)
- [timm: getting started](https://rwightman.github.io/pytorch-image-models/)
- [timm: overview](https://fastai.github.io/timmdocs/)
- [Pytorch/timm tutorial for transfer learning](https://www.kaggle.com/hinepo/transfer-learning-with-timm-models-and-pytorch) - @hinepo

### Papers

- [AlexNet - paper sep/2012](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [ResNet - paper dec/2015](https://arxiv.org/pdf/1512.03385.pdf)
- [ResNeXt - paper apr/2017](https://arxiv.org/pdf/1611.05431.pdf)
- [Squeeze-and-Excitation Networks - SE ResNet - paper may/2019](https://arxiv.org/pdf/1709.01507v4.pdf)
- [Self-Training with Noisy Student - paper jun/2020](https://arxiv.org/pdf/1911.04252.pdf)
- [EfficientNet - paper sep/2020](https://arxiv.org/pdf/1905.11946.pdf)
- [Meta Pseudo Labels - paper mar/2021](https://arxiv.org/pdf/2003.10580v4.pdf)
- [A ConvNet fot the 2020s - ConvNext paper jan/2022](https://arxiv.org/pdf/2201.03545.pdf)

### Vision Transformer (ViT) and others

- [ViT - 5 min video](https://www.youtube.com/watch?v=DVoHvmww2lQ)
- [ViT - Hugging Face](https://huggingface.co/docs/transformers/model_doc/vit)
- [ViT exemplo - tranformer library](https://www.youtube.com/watch?app=desktop&v=Bjp7hebC67E)
- [ViT exemplo - timm library](https://www.youtube.com/watch?v=ovB0ddFtzzA)
- [Swin Transformer](https://www.youtube.com/watch?v=SndHALawoag)

# RecSys

- [OTTO competition](https://www.kaggle.com/competitions/otto-recommender-system) é o melhor e mais completo material sobre sistemas de recomendação, incluindo muitas abordagens, baselines, códigos, discussões e soluções.
- [EDA for e-commerce RecSys: Matrix Factorization + TSNE](https://www.kaggle.com/code/cdeotte/user-eda-with-rapids-tsne-and-matrix-factorization)


### Recommendation systems basic concepts:

- [An introduction to Recommender Systems -  a thread 🧵 - Radek Osmulski](https://twitter.com/radekosmulski/status/1631513789484126208)
- [Recommender Systems in Python 101](https://www.kaggle.com/code/gspmoreira/recommender-systems-in-python-101/notebook)
- [Teoria - Khrish Naik](https://www.youtube.com/watch?v=EjOlN6uVBOg)
- [Prática - Khrish Naik](https://www.youtube.com/watch?v=R64Lh1Qwl_0)
- [RecSys with KNN - Khrish Naik](https://www.youtube.com/watch?v=kccT0FVK6OY)
- [Outros vídeos da playlist de RecSys - Krish Naik](https://www.youtube.com/watch?v=_hf_y-_sj5Y&list=PLZoTAELRMXVN7QGpcuN-Vg35Hgjp3htvi)
- [Arquitetura de Sistemas de recomendação - Mario Filho](https://www.youtube.com/watch?v=2LbZRaRfGqk&t=2688s)
- [Etsy RecSys: from many GBDT models to one NN](https://www.etsy.com/codeascraft/how-we-built-a-multi-task-canonical-ranker-for-recommendations-at-etsy)


### Validating a recommendation model:

- [How to validate a recommendation model](https://www.kaggle.com/code/radek1/a-robust-local-validation-framework)
- [About validation](https://www.kaggle.com/competitions/otto-recommender-system/discussion/364991)
- [About spliting the data](https://github.com/otto-de/recsys-dataset)
- [Script to split the data - exemplo](https://github.com/otto-de/recsys-dataset/blob/main/src/testset.py)
- [H&M Personalized Fashion Recommendations - How To Setup Local CV](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations/discussion/308919)


### Algumas abordagens:

- Content based (item similarity / item colaborative filtering)
- User based (user similarity / user colaborative Filtering)
- [Word2Vec, FastText](https://www.kaggle.com/code/radek1/word2vec-how-to-training-and-submission)
  - train from scratch on item sequences
  - use [Annoy lib](https://github.com/spotify/annoy) to find nearest neighbors
- [Co-visitation matrix + heuristic re-ranker (rule-based ranker)](https://www.kaggle.com/code/cdeotte/candidate-rerank-model-lb-0-575)
- [Ranking models (learning to rank)](https://www.kaggle.com/competitions/otto-recommender-system/discussion/370210)
  - user features
  - item features
  - user-item interactions features
  - co-visitation features
- [Matrix Factorization](https://www.kaggle.com/code/radek1/matrix-factorization-pytorch-merlin-dataloader) & [Matrix Factorization with GPU](https://www.kaggle.com/code/cpmpml/matrix-factorization-with-gpu)
- Autoencoder
- LSTM
- Transformers
- GNN (Graph Neural Networks)
- [RecBole lib](https://recbole.io/docs/)


# Marketing Mix Modeling
- [Curso 2h](https://www.linkedin.com/learning/marketing-attribution-and-mix-modeling/measuring-marketing-performance?resume=false)
  - Multi-touch Attribution Models
  - Marketing Mix Modeling
  - Incrementality and A/B testing
- [Lightweightmmm (Bayesian) Marketing Mix Modeling](https://github.com/google/lightweight_mmm) and [Lightweightmmm Models](https://lightweight-mmm.readthedocs.io/en/latest/models.html)
- [Artigos de MMM do Mario Filho](https://forecastegy.com/tags/marketing-mix-modeling/)
  - [Marketing Mix Model With LightweightMMM (Python)](https://forecastegy.com/posts/how-to-create-a-marketing-mix-model-with-lightweightmmm/)
  - [What I Learned Watching 7 Hours of Meta's Marketing Mix Modeling Summits](https://forecastegy.com/posts/marketing-mix-modeling-meta-summit/)
  - [Adstock in Marketing Mix Modeling](https://forecastegy.com/posts/adstock-in-marketing-mix-modeling/)
  - [Case Study: Marketing Mix Modeling with Social Media And Google](https://forecastegy.com/posts/marketing-mix-modeling-case-study-social-media-google/)
  - [Don't Let Multicollinearity Mess Up Your Marketing Mix Model](https://forecastegy.com/posts/dont-let-multicollinearity-mess-up-your-marketing-mix-model/)


# Survival Analysis
- Soon...


# Deploy

- [Deploy pipelines: Feature, Training, Inference](https://www.linkedin.com/feed/update/urn:li:activity:7076483314697609216/)
- [Data Roles and Data Value Chain](https://www.linkedin.com/feed/update/urn:li:activity:7076423576031027201/)
- [4 Types of ML model deployment](https://twitter.com/Aurimas_Gr/status/1656959857205936129)
- [A/B Testing](https://www.kaggle.com/code/ekrembayar/a-b-testing-step-by-step-hypothesis-testing)
- [Streamlit - Revolutionizing Data App Creation](https://pub.towardsai.net/streamlit-revolutionizing-data-app-creation-e269177d9112)
- [Use docker for deployment, not for development](https://twitter.com/radekosmulski/status/1615856695879798784)


### Capítulos do livro "Designing Machine Learning Systemns - Chip Huyen":

- 7 - Model Deployment and Prediction Service
  - Offline evaluation
  - Backtesting
  - Batch x Online
  - Cloud x Edge
- 8 - Data Distribution Shifts and Monitoring
  - How to monitor raw inputs, features, predictions, metrics
- 9 - Continual Learning and Test in Production
  - Online evaluation
  - Shadow Deployment
  - A/B Testing
  - Canary release
  - Interleaving experiments
  - Bandits


### Deploying supervised machine learning models

Deploying is an intervention and humans are agents that react to the predictions. That is the point of most models.

- [Performative prediction](https://twitter.com/ChristophMolnar/status/1569644089724764160)


# MLOps

Intro:
- [Como é o trabalho de um ML Enginner (MLE)?](https://www.youtube.com/watch?v=xCcrMhwzhpI)
- Capítulo "10 - Infrastructure and Tooling for MLOps" do livro "Designing Machine Learning Systemns - Chip Huyen"
- [Made With ML](https://madewithml.com/)
- [Machine Learning Operations](https://ml-ops.org/)
- [MyMLOps - Build your MLOps stack](https://mymlops.com/)
- [MLOps: Overview, Definition, and Architecture - paper mai/2022](https://arxiv.org/ftp/arxiv/papers/2205/2205.02302.pdf)
- [Operationalizing Machine Learning: An Interview Study - paper sep/2022](https://arxiv.org/pdf/2209.09125.pdf)

Tools & References:
- [A curated list of awesome MLOps tools - kelvins](https://github.com/kelvins/awesome-mlops)
- [A curated list of references for MLOps - visenger](https://github.com/visenger/awesome-mlops)

Model Store:
- [MLflow](https://www.youtube.com/watch?app=desktop&v=859OxXrt_TI)
- [MLflow on Databricks](https://www.youtube.com/watch?app=desktop&v=nx3yFzx_nHI&t=1260s)
- [Weights and biases](https://www.youtube.com/watch?v=krWjJcW80_A)


# Youtube channels

Abaixo alguns canais nos quais acho válido se inscrever e acompanhar os conteúdos publicados.

- [Abhishek Thakur](https://www.youtube.com/c/AbhishekThakurAbhi)
- [AI Coffee Break with Letitia](https://www.youtube.com/c/AICoffeeBreak)
- [Chai Time Data Science](https://www.youtube.com/c/ChaiTimeDataScience)
- [Medallion Data Science](https://www.youtube.com/channel/UCxladMszXan-jfgzyeIMyvw/videos)
- [Krish Naik](https://www.youtube.com/user/krishnaik06)
- [Underfitted](https://www.youtube.com/c/Santiagox0)
- [Andrej Karpathy](https://www.youtube.com/c/AndrejKarpathy/videos)
- [BI Elite](https://www.youtube.com/c/BIElite)


# Perfis no twitter

Algumas sugestões:

- [Chip Huyen](https://twitter.com/chipro)
- [Mark Tenenholtz](https://twitter.com/marktenenholtz)
- [Santiago](https://twitter.com/svpino)
- [François Chollet](https://twitter.com/fchollet)
- [Bojan Tunguz](https://twitter.com/tunguz)
- [abhishek](https://twitter.com/abhi1thakur)
- [Aurimas Griciūnas](https://twitter.com/Aurimas_Gr)
- [Konrad Banachewicz](https://twitter.com/tng_konrad)
- [JFPuget](https://twitter.com/JFPuget)
- [Chris Deotte](https://twitter.com/ChrisDeotte)
- [Goku Mohandas](https://twitter.com/GokuMohandas)
- [Yann Lecun](https://twitter.com/ylecun)
- [Andrew Ng](https://twitter.com/AndrewYNg)
- [arXiv abstract](https://twitter.com/arxivabs)
- [Sanyam Bhutani](https://twitter.com/bhutanisanyam1)
- [Ross Wightman](https://twitter.com/wightmanr)
- [Hugging Face](https://twitter.com/huggingface)
- [Harrison Chase](https://twitter.com/hwchase17)
- [AI Coffee Break with Letitia Parcalabescu](https://twitter.com/AICoffeeBreak)
- [Andrej Karpathy](https://twitter.com/karpathy)
- [Philipp Singer](https://twitter.com/ph_singer)
- [Andrada Olteanu](https://twitter.com/andradaolteanuu)
- [Jeremy Howard](https://twitter.com/jeremyphoward)
- [Ultralytics](https://twitter.com/ultralytics)
- [Harrison Kinsley](https://twitter.com/Sentdex)
- [Martin Henze (Heads or Tails)](https://twitter.com/heads0rtai1s)
- [Anthony Goldbloom](https://twitter.com/antgoldbloom)
- [Machine Learning Mastery](https://twitter.com/TeachTheMachine)



