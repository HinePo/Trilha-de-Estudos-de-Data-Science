# Trilha de Estudos de Ci√™ncia de Dados

Este √© um reposit√≥rio em constante constru√ß√£o e atualiza√ß√£o. Adiciono aqui t√©cnicas de estudo e fontes que considero boas para o aprendizado de ci√™ncia de dados, com o objetivo de manter recursos organizados para consulta e ajudar quem se interessa pelo tema. O conte√∫do aqui compilado vai do b√°sico ao avan√ßado.

# Sum√°rio
- [Como estudar](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#como-estudar)
- [Ferramentas](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#ferramentas)
- [Python and Data Analysis basics](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#python-and-data-analysis-basics)
- [Data Visualization](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#data-visualization)
- [Machine Learning - Teoria](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#machine-learning---teoria)
- [Machine Learning - Pr√°tica](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#machine-learning---pr√°tica)
- [Time Series](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#time-series)
- [Deep Learning - Neural Networks](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#deep-learning---neural-networks)
- [Transformers](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#transformers)
- [NLP - Natural Language Processing](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#nlp---natural-language-processing)
- [Computer Vision](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#computer-vision)
- [MLOps](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#mlops)
- [Youtube channels](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#youtube-channels)
- [Perfis no twitter](https://github.com/HinePo/Trilha-de-Estudos-de-Data-Science/blob/main/README.md#perfis-no-twitter)

# Como estudar
- Criar um doc (word) pessoal com a sua organiza√ß√£o do que vc j√° aprendeu/estudou e o que planeja aprender/estudar, de prefer√™ncia organizado por m√™s ou bimestre. Procurar manter este doc atualizado, se poss√≠vel;
- Instalar a extens√£o [video speed controller](https://chrome.google.com/webstore/detail/video-speed-controller/nffaoalbilbmmfgbnbgppjihopabppdk) no google chrome (funciona em qualquer v√≠deo tocado pelo chrome browser), e aprender a usar:

![image](https://user-images.githubusercontent.com/66163270/145697555-17f7fb51-ec8d-4f9f-8c36-654b062cddce.png)

Isso aumentar√° muito a sua produtividade.
-	Ao entrar em um assunto novo, gosto de ver um ou dois v√≠deos de ~10 min no youtube, pesquisar sobre o tema focando em material escrito, e estudar aplica√ß√µes;
-	Evitar ficar muito tempo na parte te√≥rica: Qualquer assunto novo tem suas aplica√ß√µes, via bibliotecas espec√≠ficas. Se familiarizar com a documenta√ß√£o √© o primeiro passo (google ‚Äúpandas docs‚Äù, ‚Äúseaborn cheat sheet‚Äù...);
-	O segundo passo √© a aplica√ß√£o e uso, parte pr√°tica: Resolver problemas usando IA: Pesquisar aplica√ß√µes no Kaggle (notebooks), fazer o fork (Copy and Edit), adicionar ideias. Evitar tentar reinventar a roda: aproveitar os c√≥digos que j√° existem;
- Adicionar aplica√ß√£o ao seu reposit√≥rio pessoal (p√∫blico ou privado) de forma organizada para que voc√™ possa facilmente consult√°-la no futuro;
- O twitter (e tamb√©m o instagram, em menor escala) podem e devem ser utilizados como poderosas ferramentas de estudo e atualiza√ß√£o. Funcionam muito bem como dose di√°ria e homeop√°tica de aprendizado, e ajudam muito a acompanhar o trabalho de outras pessoas, cientistas de dados e pesquisadores. O twitter √© uma ferramenta essencial para o acompanhamento dos avan√ßos na √°rea de ci√™ncia de dados, do estado da arte e dos papers publicados, e para a absor√ß√£o de dicas e experi√™ncias compartilhadas sobre casos reais de DS na ind√∫stria e √°rea de neg√≥cios. Seguir #‚Äôs dos assuntos que te interessam tamb√©m ajuda muito a se manter atualizado, j√° que em ci√™ncia de dados as coisas acontecem/andam/mudam muito r√°pido e plataformas que te entregam a informa√ß√£o de forma r√°pida, resumida e eficiente costumam ser muito √∫teis. Ver sugest√µes de perfis a seguir no final deste documento.

# Ferramentas
- Focar em Google Colab e Kaggle notebooks.
- No futuro, √© interessante conhecer IDEs como VS Code, PyCharm e Spyder.
- Sublime Text √© um √≥timo editor de c√≥digo.

# Python and Data Analysis basics
- [Never memorize code](https://www.youtube.com/watch?app=desktop&v=AavXBoxTCIA) - v√≠deo
- [How to learn data science smartly](https://www.youtube.com/watch?app=desktop&v=csG_qfOTvxw) - v√≠deo
- [Learn Pandas with pokemons](https://www.kaggle.com/ash316/learn-pandas-with-pokemons)
- [Pandas docs](https://pandas.pydata.org/docs/index.html)
- [Handling Missing Data](https://www.kaggle.com/robikscube/handling-with-missing-data-youtube-stream)
- [Python projects](https://medium.com/coders-camp/180-data-science-and-machine-learning-projects-with-python-6191bc7b9db9)

### Data Analysis workflow - entender e praticar as etapas b√°sicas:

- Importar e ler csv, criar dataframe
- Checar tipos de vari√°veis (data types): num√©ricas e categ√≥ricas
- Preprocess: T√©cnicas para lidar com vari√°veis categ√≥ricas: one-hot encoding, label encoding, ordinal encoding....
- Plots b√°sicos
- Analisar missing values (valores faltantes), tomar decis√µes sobre o que fazer com eles
- Analisar outliers, decidir o que fazer com eles
- An√°lise univariada, bivariada, multivariada
- Feature Engineering (cria√ß√£o de vari√°veis)
- Deixar dados prontos para eventual modelagem de IA

### Machine Learning workflow - entender e praticar as etapas b√°sicas:

- Split train/validation datasets
- Definir Features and Target (if it is a supervised problem)
- Preprocess: Scaling and categorical encoders
- Check Target distributions
- Check features distributions, normalize them if needed
- Definir m√©tricas de avalia√ß√£o dos modelos
- Choose algorithm, Train model
- Evaluate model
- Melhorar modelo, tunar hiperpar√¢metros, treinar de novo, avaliar de novo
- Experimentos com diferentes sets de features
- Ensemble: combinar modelos para aumentar performance e poder de generaliza√ß√£o

# Data Visualization
- [A Simple Tutorial To Data Visualization](https://www.kaggle.com/vanshjatana/a-simple-tutorial-to-data-visualization#notebook-container) - @vanshjatana
- [S√©ries de notebooks de visualiza√ß√£o](https://www.kaggle.com/residentmario/univariate-plotting-with-pandas) - ao final de cada notebook tem um link para o pr√≥ximo
- [Data Analysis - Brazilian Society (PNAD)](https://www.kaggle.com/hinepo/pnad-data-analysis) - @hinepo
- [Rio Temperature Analysis](https://www.kaggle.com/hinepo/is-rio-getting-hotter-seasons-analysis/) - @hinepo
- [Visual Reference](https://www.sqlbi.com/wp-content/uploads/videotrainings/dashboarddesign/visuals-reference-may2017-A3.pdf)
- [Power BI playlists](https://www.youtube.com/c/HashtagTreinamentos/playlists?app=desktop)
- [Power BI - Karine Lago](https://www.youtube.com/c/KarineLago/playlists?app=desktop)
- [Power BI + DAX + Projetos na pr√°tica - Curso Udemy](https://www.udemy.com/course/curso-de-powerbi-desktop-dax/)

# Machine Learning - Teoria
- [Supervised x Unsupervised Learning](https://www.youtube.com/watch?app=desktop&v=1FZ0A1QCMWc) - v√≠deo
- [Supervised x Unsupervised Learning: applications](https://www.youtube.com/watch?app=desktop&v=rHeaoaiBM6Y) - v√≠deo
- Pesquisar sobre Overfitting e Underfitting, ver v√≠deos e gr√°ficos
- [Cross Validation](https://www.youtube.com/watch?app=desktop&v=fSytzGwwBVw)
- [Cross Validation - scikit docs](https://scikit-learn.org/stable/modules/cross_validation.html)
- Pesquisar sobre Cross Validation para Time Series (como evitar contamina√ß√£o de dados do futuro pro passado, data leakage, train/test contamination...)
- [pdf do livro do Abhishek Thakur](https://github.com/abhishekkrthakur/approachingalmost/blob/master/AAAMLP.pdf) - resumo com tudo, dispon√≠vel na Amazon tb
- [Kaggle courses](https://www.kaggle.com/learn)
- [Statquest - V√≠deos sobre conceitos, teoria e matem√°tica de algoritmos e ML](https://www.youtube.com/c/joshstarmer/playlists?app=desktop)
- [Scikit-learn](https://scikit-learn.org/stable/)
- [Scikit-learn User Guide](https://scikit-learn.org/stable/user_guide.html) - Acho muito importante ler todo o item 1. Na primeira leitura n√£o precisa entender tudo com profundidade, mas tem que se familiarizar com a documenta√ß√£o do scikit, especialmente com o item 1 todo. √â uma biblioteca muito importante para se aprender a usar e consultar.
- [Scikit-learn Pre-processing](https://scikit-learn.org/stable/modules/preprocessing.html)
- [ML projects for beginners - with code](https://github.com/microsoft/ML-For-Beginners)
- [Scipy docs](https://scipy.org/) - Procurar aplica√ß√£o do pacote
- Pesquisar sobre "Feature Engineering" (cria√ß√£o de vari√°veis)
- Pesquisar sobre m√©tricas e como avaliar modelos:
  - Classifica√ß√£o: Accuracy, ROC AUC, f1-score, recall, precision
  - Regress√£o: RMSE, MSE, MAE, R¬≤
- Outros conceitos importantes: Pesquisar sobre Boosting (XGBoost, LGBM, GBM), Bagging, Split train/test, data leakage, time series, ARIMA, feature importances, ensemble...
- Imbalanced learning:
  - downsampling/upsampling
  - [Transforming skewed data](https://medium.com/@ODSC/transforming-skewed-data-for-machine-learning-90e6cc364b0) - como tratar o vi√©s nos dados
  - [imblearn](https://opendatascience.com/strategies-for-addressing-class-imbalance/)
  - [Oversampling x Undersampling](https://www.kdnuggets.com/2020/01/5-most-useful-techniques-handle-imbalanced-datasets.html)
  - [Resampling example](https://www.kaggle.com/rafjaa/resampling-strategies-for-imbalanced-datasets)
  - [SMOTE for classification example](https://www.kaggle.com/shrutimechlearn/pokemon-classification-and-smote-for-imbalance/notebook)
  - [Stop Using SMOTE to Treat Class Imbalance](https://towardsdatascience.com/stop-using-smote-to-treat-class-imbalance-take-this-intuitive-approach-instead-9cb822b8dc45)

# Machine Learning - Pr√°tica
- [Kaggle's 30 Days of ML](https://www.youtube.com/playlist?app=desktop&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N) - Abhishek Thakur
- [Applied Machine Learning](https://www.kaggle.com/vanshjatana/applied-machine-learning) - @vanshjatana
- [Data Visualization & Income Prediction for Brazilians](https://www.kaggle.com/hinepo/pnad-income-prediction) - @hinepo
- [Lazy Predict](https://www.kaggle.com/hinepo/pnad-lazy-predict) - @hinepo
- [Heart disease and ensembling](https://www.kaggle.com/hinepo/ensemble-to-predict-diabetes-100-acc-on-val-data) - @hinepo
- Browse kaggle, ver notebooks e datasets dos assuntos que te interessam
- Fazer forks de notebooks do kaggle (Copy and Edit), testar hip√≥teses e t√©cnicas
- Falar com as pessoas do kaggle, comentar e postar, fazer parte da comunidade
- Competi√ß√µes 'Getting Started': estudar notebooks com bom score, e usar t√©cnicas e conceitos aprendidos para criar o seu pr√≥prio. Estudar notebooks com score m√©dio, comparar com os de score bom, e entender o que causou a melhora na pontua√ß√£o. Recomendo no m√≠nimo uns 10 dias de estudo para cada uma das competi√ß√µes abaixo:
  - [Titanic Classification](https://www.kaggle.com/c/titanic)
  - [House Prices Regression](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
  - [Predict Future Sales](https://www.kaggle.com/c/competitive-data-science-predict-future-sales)
  - [Tabular Playground Series](https://www.kaggle.com/c/tabular-playground-series-sep-2021)
  - N√≠vel avan√ßado: competi√ß√µes reais (valendo pr√™mios)


### Algorithm Optimization & Tuning techniques

- [Optuna library](https://optuna.org/)
- [Optuna example - notebook](https://www.kaggle.com/hinepo/extra-trees-optimization-with-optuna) - @hinepo
- [Optuna example](https://www.youtube.com/watch?v=m5YSKPMjkrk&list=PL98nY_tJQXZnP-k3qCDd1hljVSciDV9_N&index=23) - Abhishek Thakur
- [Optuna official tutorial](https://optuna.readthedocs.io/en/stable/tutorial/index.html)
- [Tuning techniques](https://www.youtube.com/watch?v=5nYqK-HaoKY) - Abhishek Thakur


# Time Series

- [Time Series - Youtube playlist](https://www.youtube.com/playlist?list=PL98nY_tJQXZmT9ZB59T0lsx0ZzzLrYdX4)
- [Error Analysis for Time Series - twitter thread](https://twitter.com/marktenenholtz/status/1509500787189190658)


# Deep Learning - Neural Networks
Principais conceitos e keywords a pesquisar e aprender: tensors, gradient descent, automatic differentiation, forward pass, backpropagation, layers, vanishing gradients, exploding gradients, transfer learning (fine-tuning & feature extraction)...

- [Fine-tuning x Feature Extraction - pytorch docs and examples](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)

"In finetuning, we start with a pretrained model and update all of the model‚Äôs parameters for our new task, in essence retraining the whole model. In feature extraction, we start with a pretrained model and only update the final layer weights from which we derive predictions."

- [Neural Networks - 3Blue1Brown Playlist (~1h)](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)
- [Aula Intro de DL - Lex Friedman](https://www.youtube.com/watch?app=desktop&v=O5xeyoRL95U) - v√≠deo
- [Pytorch vs Tensorflow in 2022](https://www.assemblyai.com/blog/pytorch-vs-tensorflow-in-2022/) - tend√™ncias
- [Keras docs](https://keras.io/)
- [Keras Sequential class](https://keras.io/api/models/sequential/)
- [Keras - code examples](https://keras.io/examples/)
- [Tensorflow docs](https://www.tensorflow.org/)
- [Pytorch docs - getting started](https://pytorch.org/get-started/locally/)
- [Pytorch - Abhishek Thakur playlist and tutorials](https://www.youtube.com/playlist?app=desktop&list=PL98nY_tJQXZln8spB5uTZdKN08mYGkOf2)
- [Pytorch - torch.nn](https://pytorch.org/docs/stable/nn.html)

Um estudo muito √∫til e proveitoso √© comparar e olhar em paralelo as documenta√ß√µes de Quick Start do Keras, do Tensorflow e do Pytorch. A l√≥gica √© bem parecida e existem muitas analogias:
- [Keras Quick Start](https://www.tensorflow.org/tutorials/quickstart/beginner)
- [Tensorflow Quick Start](https://www.tensorflow.org/tutorials/quickstart/advanced)
- [Pytorch Quick Start](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)

Principais tipos de camadas (layers):
- Dense & Linear (fully connected)
- Activation functions (ReLU, LeakyReLU, SELU, PReLU, Tanh, Softmax....)
- Conv (Convolutional)
- Flatten
- Batch Normalization
- LSTM (Long Short Term Memory)
- GRU (Gated Recurrent Unit - Short Term Memory)
- Dropout
- Pooling

### Papers - Why and When Deep Learning?

- [Do We Really Need Deep Learning Models for Time Series Forecasting? - paper oct/2021](https://arxiv.org/pdf/2101.02118.pdf)
- [Tabular Data: Deep Learning Is Not All You Need - paper nov/2021](https://arxiv.org/pdf/2106.03253.pdf)

### Extra:

- [JAX docs](https://jax.readthedocs.io/en/latest/) 

"JAX is Autograd and XLA, brought together for high-performance numerical computing and machine learning research. It provides composable transformations of Python+NumPy programs: differentiate, vectorize, parallelize, Just-In-Time compile to GPU/TPU, and more."

- [JAX - Quick Start](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html)

"JAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research."

[JAX √© um projeto open source do Google](https://github.com/google/jax) com o objetivo de criar uma API simples e backend eficiente para c√°lculos de deep learning. Tem crescido em popularidade e sido considerada muito promissora por pesquisadores. Imagina-se que em alguns anos ser√° um concorrente direto do Pytorch (na √°rea de pesquisa), e tamb√©m dever√° substituir o backend do tensorflow (na √°rea de aplica√ß√µes). H√° quem chame o JAX de  "tensorflow 3", e j√° existem planos para cria√ß√£o de uma API high level para JAX, adaptando a biclioteca Keras para usar JAX como backend. Portanto, √© interessante conhecer.


# Transformers
Os Transformers e o Attention Mechanism, propostos em 2017 por Vaswani - Google Brain no paper Attention Is All You Need, s√£o, at√© hoje, a maior revolu√ß√£o que o mundo do Deep Learning j√° passou. Vale a pena estud√°-los com aten√ß√£o (pun intended üòÜ), pois eles s√£o o estado da arte em redes neurais hoje em dia para a maioria dos tasks, e pelo visto continuar√£o sendo por bastante tempo.

Transformers mostraram que n√£o √© preciso usar camadas LSTM para fazer tasks de NLP no estado da arte, e tamb√©m n√£o precisamos de camadas de Convolu√ß√£o para fazer CV (Computer Vision) no estado da arte. Attention Is All You Need.

### Papers

- [How to read papers - twitter thread](https://twitter.com/marktenenholtz/status/1498644231149142025)
- [Attention Is All You Need - paper dec/2017](https://arxiv.org/pdf/1706.03762.pdf) - paper original: fundamental ler
- [BERT - paper may/2019](https://arxiv.org/pdf/1810.04805.pdf) - BERT paper
- [RoBERTa - paper jul/2019](https://arxiv.org/pdf/1907.11692.pdf) - RoBERTa paper
- [Longformer - paper dec/2020](https://arxiv.org/pdf/2004.05150.pdf) - Longformer paper - Local Attention (SOTA transfomer model)
- [Vision Transformers (ViT) - paper jun/2021](https://arxiv.org/pdf/2010.11929.pdf) - ViT paper
- [Swin Transformer - paper aug/2021](https://arxiv.org/pdf/2103.14030.pdf) - Swin Transformer paper - Shifted Window based Self-Attention
- [DeBERTa: Disentangled Attention - paper oct/2021](https://arxiv.org/pdf/2006.03654.pdf) - DeBERTa paper


### Outras fontes

- [BERT Attention Mechanism](https://peltarion.com/blog/data-science/self-attention-video) - v√≠deo
- [Illustrated Guide to Transformers](https://www.youtube.com/watch?app=desktop&v=4Bdc55j80l8) - v√≠deo
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) - blog & v√≠deo
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) - explica√ß√µes sobre o paper Attention Is All You Need
- [Attention implementation in torch from scratch - twitter thread](https://twitter.com/abhi1thakur/status/1470406419786698761) - Abhishek Thakur
- [Attention implementation in torch from scratch - twitter thread 2](https://twitter.com/abhi1thakur/status/1471126502112698368) - Abhishek Thakur
- [Transformers from Scratch](https://e2eml.school/transformers.html) - explica√ß√£o visual e detalhada


# NLP - Natural Language Processing
Principais conceitos e keywords a conhecer: n-grams, CBOW (Continuous Bag of Words), Word2vec, FastText (facebook model), GloVe (Global Vectors), CountVectorizer, TF-IDF, BERT, RoBERTa, Hugging Face....

- [A brief timeline of NLP from Bag of Words to the Transformer family](https://medium.com/nlplanet/a-brief-timeline-of-nlp-from-bag-of-words-to-the-transformer-family-7caad8bbba56)
- [BERT tutorial by Abhishek Thakur](https://www.youtube.com/playlist?app=desktop&list=PL98nY_tJQXZl0WwsJluhc6tGrKWCX2suH)
- [Hugging Face course](https://huggingface.co/course/chapter1/1) - excelente curso. HF √© o melhor ecossistema de NLP e continuar√° sendo por muitos anos
- [Hugging Face tutorial - video (30 min)](https://www.youtube.com/watch?v=DQc2Mi7BcuI])
- [Text Classification from Scratch](https://www.kaggle.com/vanshjatana/text-classification-from-scratch?scriptVersionId=33686389) - @vanshjatana
- [10 Things You Need to Know About BERT and Transformer Architecture](https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape)
- [A Survey of Transformers - paper jun/2021](https://arxiv.org/pdf/2106.04554.pdf) - paper para consulta
- [Transformers in Tensorflow](https://www.tensorflow.org/text/tutorials/transformer#setup)

# Computer Vision

### Yolo

- [Yolo Introduction](https://www.youtube.com/watch?app=desktop&v=4eIBisqx9_g) - v√≠deo
- [Face mask yolo-v5 example](https://www.youtube.com/watch?v=12UoOlsRwh8) - v√≠deo
- [Yolo v5 tutorial - notebook](https://colab.research.google.com/github/ultralytics/yolov5/blob/master/tutorial.ipynb)
- [Yolo tutorial by Abhishek Thakur](https://www.youtube.com/watch?app=desktop&v=NU9Xr_NYslo) - v√≠deo
- [Yolo v5 github repo - ultralytics](https://github.com/ultralytics/yolov5)
- [Yolo v5 - paper jul/2021](https://arxiv.org/pdf/2104.13634.pdf)

### Basics

- [Kernel size in convolution layers](https://medium.com/analytics-vidhya/significance-of-kernel-size-200d769aecb1)
- [Digit Recognizer: Getting Started Competition](https://www.kaggle.com/c/digit-recognizer) - ‚ÄòHello World‚Äô do mundo de CV: Estudar v√°rios notebooks com bom score, e depois criar o seu misturando v√°rias t√©cnicas que vc achou promissoras em outros notebooks, tentando melhorar o score do baseline. Recomendo no m√≠nimo uns 10 dias de estudo para essa competi√ß√£o.
- [Pytorch tutorial for image classification](https://www.kaggle.com/hinepo/pytorch-tutorial-cv-99-67-lb-99-26) - @hinepo
- [Ensemble for image classification](https://www.kaggle.com/hinepo/ensemble-by-probabilities-lb-99-43) - @hinepo 

### Pytorch image models (timm)

O que o Hugging Face √© para NLP √© an√°logo ao que a biblioteca timm √© para computer vision: um ecossistema open source, consolidado e no estado da arte, que disponibiliza uma API simples e unificada para uso de modelos, al√©m de centenas de excelentes modelos multi-prop√≥sito (multi-task, general purpose models), j√° pr√©-treinados durante semanas em GPUs e TPUs de dezenas de milhares de d√≥lares, todos prontos para usarmos apenas adicionando uma √∫ltima camada na rede neural para atender ao nosso task/problema. Isso se chama feature extraction, e evita que tenhamos que treinar esses modelos gigantes from scratch.

- [timm tutorial](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055)
- [timm: pytorch image models](https://github.com/rwightman/pytorch-image-models/tree/master/timm)
- [timm: getting started](https://rwightman.github.io/pytorch-image-models/)
- [timm: overview](https://fastai.github.io/timmdocs/)
- [Pytorch/timm tutorial for transfer learning](https://www.kaggle.com/hinepo/transfer-learning-with-timm-models-and-pytorch) - @hinepo

### Papers

- [AlexNet - paper sep/2012](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)
- [ResNet - paper dec/2015](https://arxiv.org/pdf/1512.03385.pdf)
- [ResNeXt - paper apr/2017](https://arxiv.org/pdf/1611.05431.pdf)
- [Squeeze-and-Excitation Networks - SE ResNet - paper may/2019](https://arxiv.org/pdf/1709.01507v4.pdf)
- [Self-Training with Noisy Student - paper jun/2020](https://arxiv.org/pdf/1911.04252.pdf)
- [EfficientNet - paper sep/2020](https://arxiv.org/pdf/1905.11946.pdf)
- [Meta Pseudo Labels - paper mar/2021](https://arxiv.org/pdf/2003.10580v4.pdf) - Semi-Supervised Image Classification, Meta-Learning, Teacher-Student architecture

"Pseudo Labels methods work by having a pair of networks, one as a teacher and one as a student. The teacher generates pseudo labels on unlabeled images. These pseudo labeled images are then combined with labeled images to train the student. Thanks to the abundance of pseudo labeled data and the use of regularization methods such as data augmentation, the student learns to become better than the teacher." 

"Key to Meta Pseudo Labels is the idea that the teacher learns from the student‚Äôs feedback to generate pseudo labels in a way that best helps student‚Äôs learning."

Meta Pseudo Labels utilizes the feedback from the student to inform the teacher to generate better pseudo labels, reducing bias and inaccuracies.

- [A ConvNet fot the 2020s - ConvNext paper jan/2022](https://arxiv.org/pdf/2201.03545.pdf)

### Vision Transformer (ViT) and others

- [ViT - 5 min video](https://www.youtube.com/watch?v=DVoHvmww2lQ)
- [ViT - Hugging Face](https://huggingface.co/docs/transformers/model_doc/vit)
- [ViT exemplo - tranformer library](https://www.youtube.com/watch?app=desktop&v=Bjp7hebC67E) - v√≠deo
- [ViT exemplo - timm library](https://www.youtube.com/watch?v=ovB0ddFtzzA) - v√≠deo
- [Swin Transformer](https://www.youtube.com/watch?v=SndHALawoag) - v√≠deo


# MLOps
- [Made With ML](https://madewithml.com/) - website
- [Machine Learning Operations](https://ml-ops.org/) - website
- [MLflow](https://www.youtube.com/watch?app=desktop&v=859OxXrt_TI) - v√≠deo
- [MLflow on Databricks](https://www.youtube.com/watch?app=desktop&v=nx3yFzx_nHI&t=1260s) - v√≠deo
- [Weights and biases](https://www.youtube.com/watch?v=krWjJcW80_A) - v√≠deo


# Youtube channels

Abaixo alguns canais nos quais acho v√°lido se inscrever e acompanhar os conte√∫dos publicados.

- Abhishek Thakur
- AI Coffee Break with Letitia
- Chai Time Data Science
- Krish Naik
- BI Elite
- Yannic Kilcher
- Sentdex


# Perfis no twitter

Algumas sugest√µes:

- [JFPuget](https://twitter.com/JFPuget)
- [Mark Tenenholtz](https://twitter.com/marktenenholtz)
- [Fran√ßois Chollet](https://twitter.com/fchollet)
- [Bojan Tunguz](https://twitter.com/tunguz)
- [abhishek](https://twitter.com/abhi1thakur)
- [Konrad Banachewicz](https://twitter.com/tng_konrad)
- [Chris Deotte](https://twitter.com/ChrisDeotte)
- [Andrew Ng](https://twitter.com/AndrewYNg)
- [https://twitter.com/arxivabs](https://twitter.com/arxivabs)
- [Sanyam Bhutani](https://twitter.com/bhutanisanyam1)
- [Ross Wightman](https://twitter.com/wightmanr)
- [Hugging Face](https://twitter.com/huggingface)
- [ptrblck](https://twitter.com/ptrblck_de)
- [AI Coffee Break with Letitia Parcalabescu](https://twitter.com/AICoffeeBreak)
- [Yannic Kilcher, Tech Sister](https://twitter.com/ykilcher)
- [Philipp Singer](https://twitter.com/ph_singer)
- [Andrada Olteanu](https://twitter.com/andradaolteanuu)
- [Santiago](https://twitter.com/svpino)
- [Jeremy Howard](https://twitter.com/jeremyphoward)
- [Mr_KnowNothing](https://twitter.com/singh_tanul)
- [Ultralytics](https://twitter.com/ultralytics)
- [Harrison Kinsley](https://twitter.com/Sentdex)
- [Martin Henze (Heads or Tails)](https://twitter.com/heads0rtai1s)
- [Anthony Goldbloom](https://twitter.com/antgoldbloom)
- [Bhavesh Bhatt](https://twitter.com/_bhaveshbhatt)
- [Machine Learning Mastery](https://twitter.com/TeachTheMachine)



